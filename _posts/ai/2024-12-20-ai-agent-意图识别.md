

# AI Agent 第一步 意图识别

意图识别（Intent Recognition）是智能助手（AI Agent）中的一个关键步骤，用于确定用户输入的问题或指令属于哪个意图类别。目前，主要是以下几种方法。


## **1. 使用预训练语言模型（如 BERT、GPT）进行意图分类**

预训练语言模型（如 BERT 或 GPT 系列）具有强大的语言理解能力，可以通过少量的标注数据和微调（fine-tuning）来实现高效的意图分类。

### 工作流程
1. 使用已有的标注数据集（问题和意图标签）进行训练。
2. 将问题输入到模型中，让模型学习语言中的语义关系，而不是仅仅依赖关键词。
3. 模型根据上下文和语义输出意图类别。

### 示例：使用 BERT 微调
#### 数据集格式：
```json
[
    {"text": "某公司2022年11月2日的涨跌幅是多少？", "intent": "query_stock"},
    {"text": "帮我算一下某公司的RSI指标。", "intent": "query_technical_indicator"},
    {"text": "科技行业和金融行业哪个更赚钱？", "intent": "compare_industry"},
    {"text": "你觉得今天的天气怎么样？", "intent": "small_talk"}
]
```

#### 使用 Hugging Face `transformers` 微调
```python
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
import torch

# 数据准备
texts = [
    "某公司2022年11月2日的涨跌幅是多少？",
    "帮我算一下某公司的RSI指标。",
    "科技行业和金融行业哪个更赚钱？",
    "你觉得今天的天气怎么样？"
]
labels = [0, 1, 2, 3]  # 意图类别对应的索引

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
encodings = tokenizer(texts, truncation=True, padding=True, max_length=128, return_tensors="pt")

# 转换成 PyTorch 数据集
class IntentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

dataset = IntentDataset(encodings, labels)

# 模型初始化
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=4)

# 定义训练参数
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=1
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset
)

# 开始训练
trainer.train()
```

#### 使用模型预测
```python
def predict_intent(text, model, tokenizer):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    outputs = model(**inputs)
    logits = outputs.logits
    predicted_intent = torch.argmax(logits, dim=1).item()
    return predicted_intent

user_input = "帮我算一下某公司的RSI指标。"
print(predict_intent(user_input, model, tokenizer))  # 输出: 1 (query_technical_indicator)
```

---

## **2. 使用语义相似性模型（Sentence Embedding）**

通过语义嵌入（sentence embedding）技术，可以将问题转换为语义向量，再通过相似度计算匹配到最近的意图类别。

### 工作流程
1. **语义表示**：使用预训练模型（如 Sentence-BERT 或 Universal Sentence Encoder）将输入问题和意图描述转换为向量。
2. **相似度计算**：计算输入问题与每个意图描述的语义相似度，选择最相似的意图。

### 示例：使用 Sentence-BERT
#### 安装 Sentence-BERT
```bash
pip install sentence-transformers
```

#### 示例代码
```python
from sentence_transformers import SentenceTransformer, util

# 加载 Sentence-BERT 模型
model = SentenceTransformer('all-MiniLM-L6-v2')

# 定义意图和描述
intents = {
    "query_stock": "查询某只股票的相关信息",
    "query_technical_indicator": "计算某只股票的技术指标",
    "compare_industry": "比较不同行业的表现",
    "small_talk": "普通的闲聊问题"
}

# 将意图描述转换为嵌入向量
intent_descriptions = list(intents.values())
intent_embeddings = model.encode(intent_descriptions)

# 用户输入
user_input = "帮我算一下某公司的MACD指标"
input_embedding = model.encode(user_input)

# 计算语义相似度
similarities = util.cos_sim(input_embedding, intent_embeddings)
predicted_intent = list(intents.keys())[similarities.argmax()]

print(f"用户输入: {user_input}")
print(f"预测意图: {predicted_intent}")
```

不想训练或着微调模型，也可以直接使用预训练的模型。这些预训练模型可供参考：
* roberta-large-nli-stsb-mean-tokens：在STS任务上表现优异，适合需要高准确度的场景。
* bert-large-nli-stsb-mean-tokens：也是一个在STS任务上表现不错的模型。
* distilbert-base-nli-stsb-mean-tokens：如果需要一个更轻量级的模型，这个模型是一个不错的选择。
* paraphrase-multilingual-MiniLM-L12-v2：支持多语言，适合处理多种语言的文本数据。


#### 输出结果：
```
用户输入: 帮我算一下某公司的MACD指标
预测意图: query_technical_indicator
```

---

## **3. 基于规则与模型的混合方法**

将基于规则的方法（如关键词匹配）和基于机器学习的方法结合起来，利用规则覆盖简单情况，用机器学习处理复杂场景。

### 示例代码
```python
import re
from sentence_transformers import SentenceTransformer, util

# 加载预训练的语义嵌入模型
model = SentenceTransformer('all-MiniLM-L6-v2')

# 基于规则的意图识别
def rule_based_intent_recognition(text):
    if re.search(r"(涨跌幅|行情|股价)", text):
        return "query_stock"
    if re.search(r"(MACD|RSI|技术指标)", text):
        return "query_technical_indicator"
    if re.search(r"(行业|板块|增长|比较)", text):
        return "compare_industry"
    if re.search(r"(电影|天气|闲聊)", text):
        return "small_talk"
    return None

# 基于语义嵌入的意图识别
def embedding_based_intent_recognition(text, intents):
    intent_descriptions = list(intents.values())
    intent_embeddings = model.encode(intent_descriptions)
    input_embedding = model.encode(text)
    similarities = util.cos_sim(input_embedding, intent_embeddings)
    return list(intents.keys())[similarities.argmax()]

# 混合方法
def smart_intent_recognition(text):
    # 尝试规则匹配
    rule_intent = rule_based_intent_recognition(text)
    if rule_intent:
        return rule_intent

    # 如果规则匹配失败，则使用语义嵌入
    intents = {
        "query_stock": "查询某只股票的相关信息",
        "query_technical_indicator": "计算某只股票的技术指标",
        "compare_industry": "比较不同行业的表现",
        "small_talk": "普通的闲聊问题"
    }
    return embedding_based_intent_recognition(text, intents)

# 测试
user_input = "帮我算一下某公司的MACD指标。"
predicted_intent = smart_intent_recognition(user_input)
print(f"用户输入: {user_input}")
print(f"预测意图: {predicted_intent}")
```

---

## **4. 使用少样本学习或零样本学习（Few-shot/NLI Models）**

基于大语言模型（如 OpenAI GPT 系列）的少样本学习能力，可以在没有大量标注数据的情况下完成意图识别。

### 示例：基于 OpenAI GPT 的少样本学习
```python
import openai

# 定义提示（prompt）
def few_shot_intent_recognition(user_input):
    prompt = f"""
    以下是一些用户输入及其对应的意图：
    - "某公司2022年11月2日的涨跌幅是多少？" -> query_stock
    - "帮我算一下某公司的RSI指标。" -> query_technical_indicator
    - "科技行业和金融行业哪个更赚钱？" -> compare_industry
    - "你喜欢看电影吗？" -> small_talk

    用户输入: "{user_input}"
    意图是：
    """
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=prompt,
        max_tokens=10
    )
    return response["choices"][0]["text"].strip()

# 测试
user_input = "帮我分析一下近半年涨停次数最多的行业。"
predicted_intent = few_shot_intent_recognition(user_input)
print(f"用户输入: {user_input}")
print(f"预测意图: {predicted_intent}")
```

---

## **总结：选择更智能的方法**

### **优先选择方法：**
1. **语义相似性（Sentence Embedding）**：无需大量标注数据，适合冷启动。
2. **预训练模型微调（BERT、GPT）**：准确率高，但需要较多数据和计算资源。
3. **混合方法（规则+模型）**：适合渐进式开发，利用规则处理简单场景，用模型解决复杂问题。
4. **少样本学习（Few-shot）**：使用 GPT 等大模型时非常方便。

